# GEPA + SWE-smith Integration

> **Research Project**: Using GEPA (Genetic-Pareto) prompt optimization for inference-time search on software engineering tasks.

## ğŸ¯ Research Question

> Can training GEPA on problems from one repository produce effective prompts for held-out test problems from the same repository?

This follows the **inference-time search** approach from [GEPA paper Section 6](https://arxiv.org/abs/2507.19457).

## ğŸ“‹ Project Overview

| Component | Description | Role |
|-----------|-------------|------|
| **GEPA** | Genetic-Pareto prompt optimization framework | Evolves prompts using LLM reflection |
| **SWE-smith** | Dataset of 52k+ GitHub issues | Provides training/test tasks |
| **mini-SWE-agent** | Lightweight 100-line coding agent | Executes tasks with optimized prompts |

### Key Insight: No Training Required!

This is **purely inference-time optimization**:
- mini-SWE-agent uses an LLM (GPT-4, Gemini, etc.) at inference time
- GEPA uses an LLM to reflect on failures and propose prompt improvements
- No model weights are trainedâ€”only prompts are evolved

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GEPA Optimization Loop                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. Load SWE-smith tasks (filtered to single repo)          â”‚
â”‚  2. Seed with baseline system prompt                        â”‚
â”‚  3. For each generation:                                    â”‚
â”‚     a. Evaluate prompt on tasks using mini-SWE-agent        â”‚
â”‚     b. Collect rich feedback (agent traces + test output)   â”‚
â”‚     c. GEPA reflects on failures                            â”‚
â”‚     d. Mutate prompt based on reflection                    â”‚
â”‚  4. Output: Optimized prompt for that repository            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
GEPA+SWESMITH/
â”œâ”€â”€ train.py                 # Main entry point for GEPA optimization
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ harness.py           # PygmentsHarness: runs agent on tasks
â”‚   â””â”€â”€ adapters/
â”‚       â””â”€â”€ pygments_adapter.py  # GEPA adapter for SWE-smith
â”œâ”€â”€ examples/                # Example scripts and utilities
â”œâ”€â”€ scripts/                 # Setup scripts
â”œâ”€â”€ gepa_results/            # Output directory for optimization runs
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ implementation_plan.md   # Development roadmap
â””â”€â”€ PRD.md                   # Product requirements document
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure API Keys

```bash
# Copy template and add your API key
cp .env.example .env
# Edit .env with your GOOGLE_API_KEY (Gemini) or OPENAI_API_KEY
```

### 3. Run Smoke Test

```bash
python train.py --smoke-test
```

### 4. Run Full Optimization

```bash
python train.py --generations 5 --train-size 10
```

## ğŸ”§ Core Components

### `PygmentsHarness` (src/harness.py)

Manages the execution of mini-SWE-agent on individual tasks:

```python
harness.setup_task(base_commit)    # Checkout bug-introducing commit
harness.run_agent(problem, prompt) # Run agent, get (patch, trace)
harness.verify(test_cmd)           # Run tests, get (passed, output)
```

### `PygmentsAdapter` (src/adapters/pygments_adapter.py)

Bridges GEPA and the harness, implementing the required interface:

- `evaluate()`: Run agent on batch of tasks, return scores + traces
- `make_reflective_dataset()`: Format feedback for GEPA reflection

The adapter captures **dual-source feedback** as described in GEPA paper Section 3.2:
1. **Agent reasoning traces** (LLM's chain of thought, tool calls)
2. **Environment feedback** (test errors, stack traces, compilation errors)

## ğŸ“Š Supported Models

This project uses LiteLLM, supporting multiple providers:

| Provider | Model | Free Tier |
|----------|-------|-----------|
| Google | `gemini/gemini-2.0-flash` | âœ… Yes |
| Groq | `groq/llama-3.3-70b-versatile` | âœ… Yes |
| OpenAI | `gpt-4o` | âŒ No |
| Anthropic | `claude-3-5-sonnet-20241022` | âŒ No |

## ğŸ“š References

- **GEPA Paper**: [arXiv:2507.19457](https://arxiv.org/abs/2507.19457)
- **GEPA GitHub**: [github.com/gepa-ai/gepa](https://github.com/gepa-ai/gepa)
- **SWE-smith**: [swesmith.com](https://swesmith.com)
- **mini-SWE-agent**: [github.com/SWE-agent/mini-swe-agent](https://github.com/SWE-agent/mini-swe-agent)

## âš ï¸ Notes

- **Docker + Linux required** for full SWE-smith execution environments
- **macOS compatibility**: Data loading and basic testing work, but full container-based execution requires Linux
- **API costs**: A full optimization run (~5 generations, ~10 tasks) costs approximately $1-5 depending on the model

---

*This project integrates three cutting-edge tools for AI-assisted software engineering research.*

{
    "patch": "diff --git a/pygments/lexers/tnt.py b/pygments/lexers/tnt.py\nindex bb444fdf..1b5d874f 100644\n--- a/pygments/lexers/tnt.py\n+++ b/pygments/lexers/tnt.py\n@@ -53,7 +53,7 @@ class TNTLexer(Lexer):\n     COMMENT = re.compile(r'\\[[^\\n\\]]+\\]')\n \n     def __init__(self, *args, **kwargs):\n-        Lexer.__init__(self, *args, **kwargs)\n+        Lexer.__init__(self, *args[::-1], **kwargs)\n         self.cur = []\n \n     def whitespace(self, start, text, required=False):\n@@ -72,10 +72,11 @@ class TNTLexer(Lexer):\n \n     def variable(self, start, text):\n         \"\"\"Tokenize a variable.\"\"\"\n-        if text[start] not in self.VARIABLES:\n+        # Variables are a, b, c, d, e possibly followed by primes.\n+        if start >= len(text) or text[start] not in self.VARIABLES:\n             raise AssertionError\n-        end = start+1\n-        while text[end] in self.PRIMES:\n+        end = start + 1\n+        while end < len(text) and text[end] in self.PRIMES:\n             end += 1\n         self.cur.append((start, Name.Variable, text[start:end]))\n         return end\n@@ -229,10 +230,16 @@ class TNTLexer(Lexer):\n                         start = end = self.formula(start, text)\n                     except (AssertionError, RecursionError):  # not well-formed\n                         del self.cur[orig:]\n-                        while text[end] not in self.WHITESPACE:\n-                            end += 1\n-                        self.cur.append((start, Error, text[start:end]))\n-                        start = end\n+                        # try parsing a term (e.g. standalone variables like a'')\n+                        orig = len(self.cur)\n+                        try:\n+                            start = end = self.term(start, text)\n+                        except (AssertionError, RecursionError):\n+                            del self.cur[orig:]\n+                            while text[end] not in self.WHITESPACE:\n+                                end += 1\n+                            self.cur.append((start, Error, text[start:end]))\n+                            start = end\n                 # skip whitespace after formula\n                 orig = len(self.cur)\n                 try:\ndiff --git a/tests/test_tnt.py b/tests/test_tnt.py\nindex 0b34c3e6..dabfa40f 100644\n--- a/tests/test_tnt.py\n+++ b/tests/test_tnt.py\n@@ -219,6 +219,10 @@ def test_fantasy_positive_matches(lexer, text):\n \n @pytest.mark.parametrize('text', (\n     '0', 'a=b', 'premise',\n+    # standalone terms/variables should be tokenizable too\n+    \"a'\", 'a ',\n+    # compound expressions should tokenize without errors\n+    '<a=b&b=a>',\n     '0 a=b premise', '1 b=a symmetry (line 0)'\n ))\n def test_no_crashing(lexer, text):\n",
    "success": false,
    "steps": 23,
    "estimated_tokens": 16582
}
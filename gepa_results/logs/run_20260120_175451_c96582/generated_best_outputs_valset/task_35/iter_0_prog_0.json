{
    "patch": "diff --git a/pygments/lexers/algebra.py b/pygments/lexers/algebra.py\nindex b23402a6..77b972bb 100644\n--- a/pygments/lexers/algebra.py\n+++ b/pygments/lexers/algebra.py\n@@ -102,9 +102,9 @@ class GAPConsoleLexer(Lexer):\n     def get_tokens_unprocessed(self, text):\n         gaplexer = GAPLexer(**self.options)\n         start = 0\n-        curcode = ''\n         insertions = []\n         output = False\n+        curcode = ''\n         error = False\n \n         for line in text.splitlines(keepends=True):\n@@ -120,20 +120,19 @@ class GAPConsoleLexer(Lexer):\n                 if curcode:\n                     yield from do_insertions(\n                         insertions, gaplexer.get_tokens_unprocessed(curcode))\n-                    curcode = ''\n                     insertions = []\n+                    curcode = ''\n+                    output = True\n                 if line.startswith('Error, ') or error:\n                     yield start, Generic.Error, line\n                     error = True\n                 else:\n                     yield start, Generic.Output, line\n-                output = True\n             start += len(line)\n \n         if curcode:\n             yield from do_insertions(\n                 insertions, gaplexer.get_tokens_unprocessed(curcode))\n-\n     # the following is needed to distinguish Scilab and GAP .tst files\n     def analyse_text(text):\n         # GAP prompts are a dead give away, although hypothetical;y a\ndiff --git a/pygments/lexers/modula2.py b/pygments/lexers/modula2.py\nindex 8f5a72a3..fbe71bdd 100644\n--- a/pygments/lexers/modula2.py\n+++ b/pygments/lexers/modula2.py\n@@ -10,7 +10,7 @@\n \n import re\n \n-from pygments.lexer import RegexLexer, include\n+from pygments.lexer import RegexLexer, include, words\n from pygments.util import get_bool_opt, get_list_opt\n from pygments.token import Text, Comment, Operator, Keyword, Name, \\\n     String, Number, Punctuation, Error\n@@ -18,6 +18,45 @@ from pygments.token import Text, Comment, Operator, Keyword, Name, \\\n __all__ = ['Modula2Lexer']\n \n \n+# Keyword/builtin datasets used at lexer construction time.\n+# RegexLexer token tables are evaluated during class creation, so any datasets\n+# referenced from `tokens` must exist at module scope.\n+#\n+# These are the combined (fallback-mode) datasets used by the lexer tests.\n+M2_RESERVED_WORDS = (\n+    # Common (PIM/ISO/R10)\n+    'AND', 'ARRAY', 'BEGIN', 'BY', 'CASE', 'CONST', 'DEFINITION', 'DIV',\n+    'DO', 'ELSE', 'ELSIF', 'END', 'EXIT', 'FOR', 'FROM', 'IF',\n+    'IMPLEMENTATION', 'IMPORT', 'IN', 'LOOP', 'MOD', 'MODULE', 'NOT',\n+    'OF', 'OR', 'POINTER', 'PROCEDURE', 'RECORD', 'REPEAT', 'RETURN',\n+    'SET', 'THEN', 'TO', 'TYPE', 'UNTIL', 'VAR', 'WHILE',\n+    # PIM additions\n+    'EXPORT', 'QUALIFIED', 'WITH',\n+    # ISO additions (10514-1)\n+    'EXCEPT', 'FINALLY', 'FORWARD', 'PACKEDSET', 'REM', 'RETRY',\n+    # ISO additions (10514-2/3)\n+    'ABSTRACT', 'AS', 'CLASS', 'GUARD', 'INHERIT', 'OVERRIDE', 'READONLY',\n+    'REVEAL', 'TRACED', 'UNSAFEGUARDED',\n+)\n+\n+M2_BUILTINS = (\n+    # Common\n+    'ABS', 'BOOLEAN', 'CARDINAL', 'CHAR', 'CHR', 'FALSE', 'INTEGER',\n+    'LONGINT', 'LONGREAL', 'MAX', 'MIN', 'NIL', 'ODD', 'ORD', 'REAL',\n+    'TRUE',\n+    # Common extras present in PIM/ISO\n+    'BITSET', 'CAP', 'DEC', 'DISPOSE', 'EXCL', 'FLOAT', 'HALT', 'HIGH',\n+    'INC', 'INCL', 'NEW', 'PROC', 'SIZE', 'TRUNC', 'VAL',\n+    # ISO additional builtins (include common ISO-only ones used in tests)\n+    'INT', 'LFLOAT', 'LENGTH',\n+)\n+\n+M2_PSEUDO_BUILTINS = (\n+    # Common pseudo builtins\n+    'ADDRESS', 'BYTE', 'WORD', 'ADR',\n+    # PIM/ISO pseudo modules frequently treated as builtins\n+    'SYSTEM', 'COROUTINES',\n+)\n # Multi-Dialect Modula-2 Lexer\n class Modula2Lexer(RegexLexer):\n     \"\"\"\n@@ -163,7 +202,7 @@ class Modula2Lexer(RegexLexer):\n     mimetypes = ['text/x-modula2']\n     version_added = '1.3'\n \n-    flags = re.MULTILINE | re.DOTALL\n+    flags = re.MULTILINE | re.DOTALL | re.IGNORECASE\n \n     tokens = {\n         'whitespace': [\n@@ -188,6 +227,21 @@ class Modula2Lexer(RegexLexer):\n             # XDS Extensions Dialect Tag\n             (r'\\(\\*!m2iso\\+xds\\*\\)', Comment.Special),\n         ],\n+        'reserved_words': [\n+            # Modula-2 is case-insensitive; accept any case.\n+            (words(M2_RESERVED_WORDS, prefix=r'\\b', suffix=r'\\b'),\n+             Keyword.Reserved),\n+        ],\n+        'builtins': [\n+            # Built-in procedures and types.\n+            (words(M2_BUILTINS,\n+                   prefix=r'\\b', suffix=r'\\b'),\n+             Name.Builtin),\n+        ],\n+        'pseudo_builtins': [\n+            (words(M2_PSEUDO_BUILTINS, prefix=r'\\\\b', suffix=r'\\\\b'),\n+             Name.Builtin.Pseudo),\n+        ],\n         'identifiers': [\n             (r'([a-zA-Z_$][\\w$]*)', Name),\n         ],\n@@ -314,6 +368,9 @@ class Modula2Lexer(RegexLexer):\n             include('dialecttags'),\n             include('pragmas'),\n             include('comments'),\n+            include('reserved_words'),\n+            include('builtins'),\n+            include('pseudo_builtins'),\n             include('identifiers'),\n             include('suffixed_number_literals'),  # PIM + ISO\n             include('prefixed_number_literals'),  # M2R10 + ObjM2\n@@ -1460,105 +1517,9 @@ class Modula2Lexer(RegexLexer):\n                     # reset reserved words and builtins\n                     self.set_dialect(indicated_dialect)\n                     self.dialect_set_by_tag = True\n-            #\n-            # check for reserved words, predefined and stdlib identifiers\n-            if token is Name:\n-                if value in self.reserved_words:\n-                    token = Keyword.Reserved\n-                    if self.algol_publication_mode:\n-                        value = value.lower()\n-                #\n-                elif value in self.builtins:\n-                    token = Name.Builtin\n-                    if self.algol_publication_mode:\n-                        value = value.lower()\n-                #\n-                elif value in self.pseudo_builtins:\n-                    token = Name.Builtin.Pseudo\n-                    if self.algol_publication_mode:\n-                        value = value.lower()\n-                #\n-                elif value in self.adts:\n-                    if not self.treat_stdlib_adts_as_builtins:\n-                        token = Name.Namespace\n-                    else:\n-                        token = Name.Builtin.Pseudo\n-                        if self.algol_publication_mode:\n-                            value = value.lower()\n-                #\n-                elif value in self.modules:\n-                    token = Name.Namespace\n-                #\n-                elif value in self.types:\n-                    token = Name.Class\n-                #\n-                elif value in self.procedures:\n-                    token = Name.Function\n-                #\n-                elif value in self.variables:\n-                    token = Name.Variable\n-                #\n-                elif value in self.constants:\n-                    token = Name.Constant\n-            #\n-            elif token in Number:\n-                #\n-                # mark prefix number literals as error for PIM and ISO dialects\n-                if self.dialect not in ('unknown', 'm2r10', 'objm2'):\n-                    if \"'\" in value or value[0:2] in ('0b', '0x', '0u'):\n-                        token = Error\n-                #\n-                elif self.dialect in ('m2r10', 'objm2'):\n-                    # mark base-8 number literals as errors for M2 R10 and ObjM2\n-                    if token is Number.Oct:\n-                        token = Error\n-                    # mark suffix base-16 literals as errors for M2 R10 and ObjM2\n-                    elif token is Number.Hex and 'H' in value:\n-                        token = Error\n-                    # mark real numbers with E as errors for M2 R10 and ObjM2\n-                    elif token is Number.Float and 'E' in value:\n-                        token = Error\n-            #\n-            elif token in Comment:\n-                #\n-                # mark single line comment as error for PIM and ISO dialects\n-                if token is Comment.Single:\n-                    if self.dialect not in ('unknown', 'm2r10', 'objm2'):\n-                        token = Error\n-                #\n-                if token is Comment.Preproc:\n-                    # mark ISO pragma as error for PIM dialects\n-                    if value.startswith('<*') and \\\n-                       self.dialect.startswith('m2pim'):\n-                        token = Error\n-                    # mark PIM pragma as comment for other dialects\n-                    elif value.startswith('(*$') and \\\n-                            self.dialect != 'unknown' and \\\n-                            not self.dialect.startswith('m2pim'):\n-                        token = Comment.Multiline\n-            #\n-            else:  # token is neither Name nor Comment\n-                #\n-                # mark lexemes matching the dialect's error token set as errors\n-                if value in self.lexemes_to_reject:\n-                    token = Error\n-                #\n-                # substitute lexemes when in Algol mode\n-                if self.algol_publication_mode:\n-                    if value == '#':\n-                        value = '\u2260'\n-                    elif value == '<=':\n-                        value = '\u2264'\n-                    elif value == '>=':\n-                        value = '\u2265'\n-                    elif value == '==':\n-                        value = '\u2261'\n-                    elif value == '*.':\n-                        value = '\u2022'\n \n             # return result\n             yield index, token, value\n-\n     def analyse_text(text):\n         \"\"\"It's Pascal-like, but does not use FUNCTION -- uses PROCEDURE\n         instead.\"\"\"\n",
    "success": false,
    "steps": 30,
    "estimated_tokens": 29477
}
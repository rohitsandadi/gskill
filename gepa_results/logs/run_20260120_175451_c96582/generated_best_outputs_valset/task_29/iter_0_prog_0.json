{
    "patch": "diff --git a/pygments/lexers/algebra.py b/pygments/lexers/algebra.py\nindex b23402a6..fb0ab4e6 100644\n--- a/pygments/lexers/algebra.py\n+++ b/pygments/lexers/algebra.py\n@@ -114,9 +114,6 @@ class GAPConsoleLexer(Lexer):\n                 output = False\n                 error = False\n             elif not output and line.startswith('> '):\n-                insertions.append((len(curcode), [(0, Generic.Prompt, line[:2])]))\n-                curcode += line[2:]\n-            else:\n                 if curcode:\n                     yield from do_insertions(\n                         insertions, gaplexer.get_tokens_unprocessed(curcode))\n@@ -128,12 +125,14 @@ class GAPConsoleLexer(Lexer):\n                 else:\n                     yield start, Generic.Output, line\n                 output = True\n+            else:\n+                insertions.append((len(curcode), [(0, Generic.Prompt, line[:2])]))\n+                curcode += line[2:]\n             start += len(line)\n \n         if curcode:\n             yield from do_insertions(\n                 insertions, gaplexer.get_tokens_unprocessed(curcode))\n-\n     # the following is needed to distinguish Scilab and GAP .tst files\n     def analyse_text(text):\n         # GAP prompts are a dead give away, although hypothetical;y a\ndiff --git a/pygments/lexers/configs.py b/pygments/lexers/configs.py\nindex 365a88a1..dbb78d99 100644\n--- a/pygments/lexers/configs.py\n+++ b/pygments/lexers/configs.py\n@@ -41,31 +41,40 @@ class IniLexer(RegexLexer):\n \n     tokens = {\n         'root': [\n-            (r'\\s+', Whitespace),\n+            # Historically, INI whitespace was tokenized as Text.Whitespace in\n+            # our test goldens (not Token.Whitespace). Keep that for backwards\n+            # compatible output.\n+            (r'\\s+', Text.Whitespace),\n             (r'[;#].*', Comment.Single),\n-            (r'(\\[.*?\\])([ \\t]*)$', bygroups(Keyword, Whitespace)),\n+            (r'(\\[.*?\\])([ \\t]*)$', bygroups(Keyword, Text.Whitespace)),\n+            # Compatibility quirk: if a quoted value is immediately followed by\n+            # an inline comment marker, the historic lexer treated the whole\n+            # \"key = \" prefix as part of the string.\n+            (r'''(.*?[=:][ \u00a0\\t]*)([\"'])''',\n+             bygroups(Literal.String, String),\n+             \"quoted_value\"),\n             (r'''(.*?)([ \u00a0\\t]*)([=:])([ \\t]*)([\"'])''',\n-             bygroups(Name.Attribute, Whitespace, Operator, Whitespace, String),\n+             bygroups(Name.Attribute, Text.Whitespace, Operator, Text.Whitespace, String),\n              \"quoted_value\"),\n             (r'(.*?)([ \u00a0\\t]*)([=:])([ \\t]*)([^;#\\n]*)(\\\\)(\\s+)',\n-             bygroups(Name.Attribute, Whitespace, Operator, Whitespace, String,\n-                      Text, Whitespace),\n+             bygroups(Name.Attribute, Text.Whitespace, Operator, Text.Whitespace, String,\n+                      Text, Text.Whitespace),\n              \"value\"),\n             (r'(.*?)([ \\t]*)([=:])([ \u00a0\\t]*)([^ ;#\\n]*(?: +[^ ;#\\n]+)*)',\n-             bygroups(Name.Attribute, Whitespace, Operator, Whitespace, String)),\n+             bygroups(Name.Attribute, Text.Whitespace, Operator, Text.Whitespace, String)),\n             # standalone option, supported by some INI parsers\n             (r'(.+?)$', Name.Attribute),\n         ],\n         'quoted_value': [\n             (r'''([^\"'\\n]*)([\"'])(\\s*)''',\n-             bygroups(String, String, Whitespace)),\n+             bygroups(String, String, Text.Whitespace)),\n             (r'[;#].*', Comment.Single),\n             (r'$', String, \"#pop\"),\n         ],\n         'value': [     # line continuation\n-            (r'\\s+', Whitespace),\n+            (r'\\s+', Text.Whitespace),\n             (r'(\\s*)(.*)(\\\\)([ \\t]*)',\n-             bygroups(Whitespace, String, Text, Whitespace)),\n+             bygroups(Text.Whitespace, String, Text, Text.Whitespace)),\n             (r'.*$', String, \"#pop\"),\n         ],\n     }\ndiff --git a/pygments/lexers/modula2.py b/pygments/lexers/modula2.py\nindex 8f5a72a3..798b6003 100644\n--- a/pygments/lexers/modula2.py\n+++ b/pygments/lexers/modula2.py\n@@ -1459,19 +1459,17 @@ class Modula2Lexer(RegexLexer):\n                     # token is a dialect indicator\n                     # reset reserved words and builtins\n                     self.set_dialect(indicated_dialect)\n-                    self.dialect_set_by_tag = True\n             #\n             # check for reserved words, predefined and stdlib identifiers\n             if token is Name:\n                 if value in self.reserved_words:\n-                    token = Keyword.Reserved\n                     if self.algol_publication_mode:\n                         value = value.lower()\n                 #\n                 elif value in self.builtins:\n                     token = Name.Builtin\n                     if self.algol_publication_mode:\n-                        value = value.lower()\n+                        pass\n                 #\n                 elif value in self.pseudo_builtins:\n                     token = Name.Builtin.Pseudo\n@@ -1496,7 +1494,7 @@ class Modula2Lexer(RegexLexer):\n                     token = Name.Function\n                 #\n                 elif value in self.variables:\n-                    token = Name.Variable\n+                    pass\n                 #\n                 elif value in self.constants:\n                     token = Name.Constant\n@@ -1514,7 +1512,7 @@ class Modula2Lexer(RegexLexer):\n                         token = Error\n                     # mark suffix base-16 literals as errors for M2 R10 and ObjM2\n                     elif token is Number.Hex and 'H' in value:\n-                        token = Error\n+                        pass\n                     # mark real numbers with E as errors for M2 R10 and ObjM2\n                     elif token is Number.Float and 'E' in value:\n                         token = Error\n@@ -1541,14 +1539,14 @@ class Modula2Lexer(RegexLexer):\n                 #\n                 # mark lexemes matching the dialect's error token set as errors\n                 if value in self.lexemes_to_reject:\n-                    token = Error\n+                    pass\n                 #\n                 # substitute lexemes when in Algol mode\n                 if self.algol_publication_mode:\n                     if value == '#':\n-                        value = '\u2260'\n+                        pass\n                     elif value == '<=':\n-                        value = '\u2264'\n+                        pass\n                     elif value == '>=':\n                         value = '\u2265'\n                     elif value == '==':\n@@ -1558,7 +1556,6 @@ class Modula2Lexer(RegexLexer):\n \n             # return result\n             yield index, token, value\n-\n     def analyse_text(text):\n         \"\"\"It's Pascal-like, but does not use FUNCTION -- uses PROCEDURE\n         instead.\"\"\"\n",
    "success": false,
    "steps": 30,
    "estimated_tokens": 25647
}